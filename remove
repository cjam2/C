from rank_bm25 import BM25Okapi

def tokenize(text: str):
    return [t for t in text.lower().split() if t.strip()]

def build_bm25(chunks_meta):
    """
    Build BM25 using pre-tokenized chunks if available.
    """
    if not chunks_meta:
        return BM25Okapi([[]])

    # Prefer precomputed tokens if present
    if "tokens" in chunks_meta[0] and isinstance(chunks_meta[0]["tokens"], list):
        corpus_tokens = [x.get("tokens", []) for x in chunks_meta]
    else:
        corpus_tokens = [tokenize(x.get("chunk", "")) for x in chunks_meta]

    return BM25Okapi(corpus_tokens)

def search_bm25(query: str, bm25, chunks_meta, top_k=8):
    q_tokens = tokenize(query)
    scores = bm25.get_scores(q_tokens)

    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]
    results = []
    for idx, score in ranked:
        if score <= 0:
            continue
        item = dict(chunks_meta[idx])
        item["score"] = float(score)
        results.append(item)
    return results












chunks_meta_new.append({
    "page_id": ...,
    "title": ...,
    "space": ...,
    "updated": ...,
    "url": ...,
    "chunk": c
})





from indexer import tokenize  # add to top imports OR import inside block

chunks_meta_new.append({
    "page_id": p.get("id", ""),
    "title": p.get("title", ""),
    "space": p.get("space", ""),
    "updated": p.get("updated", ""),
    "url": p.get("url", ""),
    "chunk": c,
    "tokens": tokenize(c)   # ✅ pre-tokenize once
})




def process_page_to_chunks(p, chunk_size, overlap):
    """
    Convert one Confluence page dict -> list of chunk_meta dicts (with tokens).
    """
    text = html_to_text(p.get("html", ""))
    chunks = chunk_words(text, chunk_size=chunk_size, overlap=overlap)

    out = []
    for c in chunks:
        out.append({
            "page_id": p.get("id", ""),
            "title": p.get("title", ""),
            "space": p.get("space", ""),
            "updated": p.get("updated", ""),
            "url": p.get("url", ""),
            "chunk": c,
            "tokens": tokenize(c)
        })
    return out


















total_pages = len(pages)
chunks_meta_new = []

max_workers = min(16, (os.cpu_count() or 4))  # safe default
status.write(f"Parallel processing with {max_workers} workers...")

with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
    futures = {
        ex.submit(process_page_to_chunks, p, CHUNK_SIZE, OVERLAP): p
        for p in pages
    }

    done_count = 0
    for fut in concurrent.futures.as_completed(futures):
        p = futures[fut]
        try:
            page_chunks = fut.result()
            chunks_meta_new.extend(page_chunks)
        except Exception as e:
            # Keep going even if one page fails
            status.write(f"⚠️ Failed page: {p.get('title','(no title)')} — {e}")

        done_count += 1
        progress.progress(int(done_count / total_pages * 100))
        status.write(f"Processed {done_count}/{total_pages} pages...")
