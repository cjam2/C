import os, json, time, datetime
import requests
import urllib3
from requests.auth import HTTPBasicAuth

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def _sync_meta_path(data_dir: str) -> str:
    return os.path.join(data_dir, "sync_meta.json")

def _load_meta(data_dir: str) -> dict:
    p = _sync_meta_path(data_dir)
    if not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_meta(data_dir: str, meta: dict):
    with open(_sync_meta_path(data_dir), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

def _parse_confluence_when(when_str: str):
    # Example: "2026-02-27T21:30:12.123-0500"
    if not when_str:
        return None
    for fmt in ("%Y-%m-%dT%H:%M:%S.%f%z", "%Y-%m-%dT%H:%M:%S%z"):
        try:
            return datetime.datetime.strptime(when_str, fmt)
        except Exception:
            pass
    return None

def _format_cql_minute(dt: datetime.datetime) -> str:
    # Confluence CQL accepts yyyy-MM-dd HH:mm (no seconds)
    local_dt = dt.astimezone().replace(tzinfo=None)
    return local_dt.strftime("%Y-%m-%d %H:%M")

def _build_effective_cql(cfg: dict, data_dir: str) -> str:
    base = cfg["cql"].strip()
    if not cfg.get("incremental_sync", False):
        return base

    meta = _load_meta(data_dir)
    last_time = meta.get("last_sync_cql_time")      # "YYYY-MM-DD HH:MM"
    last_id = meta.get("last_sync_last_id")         # int (page id)
    if not last_time or last_id is None:
        return base

    # Tie-breaker cursor: lastmodified > time OR (lastmodified = time AND id > last_id)
    return (
        f"({base}) AND ("
        f"lastmodified > \"{last_time}\" OR "
        f"(lastmodified = \"{last_time}\" AND id > {int(last_id)})"
        f")"
    )

def sync_confluence(cfg: dict, user: str, pw: str):
    base_url = cfg["confluence_base_url"].rstrip("/")
    limit = int(cfg.get("limit", 50))

    # Make data_dir absolute and stable (avoid “different ./data” issues)
    base_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = cfg.get("data_dir", "./data")
    if not os.path.isabs(data_dir):
        data_dir = os.path.join(base_dir, data_dir)

    os.makedirs(data_dir, exist_ok=True)
    raw_dir = os.path.join(data_dir, "raw_pages")
    os.makedirs(raw_dir, exist_ok=True)

    effective_cql = _build_effective_cql(cfg, data_dir)
    auth = HTTPBasicAuth(user, pw)

    pages = []
    start = 0
    total_size = None

    # Track cursor candidates across this run
    # We want the maximum (minute, id) pair.
    max_minute = None          # string "YYYY-MM-DD HH:MM"
    max_id_in_minute = None    # int

    batch_num = 0
    max_batches = 1000  # safety
    seen_batch_first_ids = set()

    while True:
        batch_num += 1
        if batch_num > max_batches:
            raise RuntimeError("Pagination safety stop: too many batches. Likely 'start' ignored.")

        url = f"{base_url}/rest/api/content/search"
        params = {
            "cql": effective_cql,
            "start": start,
            "limit": limit,
            "expand": "body.storage,version,space,_links"
        }

        r = requests.get(
            url,
            params=params,
            auth=auth,
            headers={"Accept": "application/json"},
            timeout=60,
            verify=False  # local dev only
        )

        if r.status_code == 400:
            raise RuntimeError(f"400 from Confluence. CQL was:\n{effective_cql}\n\nResponse:\n{r.text[:800]}")

        r.raise_for_status()
        payload = r.json()

        if total_size is None:
            total_size = payload.get("totalSize")

        results = payload.get("results", [])
        size = payload.get("size", len(results))

        print(f"[SYNC] batch={batch_num} start={start} size={size} totalSize={total_size}")

        if not results:
            break

        # detect repeating batches
        first_id = results[0].get("id")
        if first_id in seen_batch_first_ids:
            raise RuntimeError(
                "Pagination loop detected: received the same first page id again. "
                f"Repeated first_id={first_id}\nEffective CQL={effective_cql}"
            )
        seen_batch_first_ids.add(first_id)

        for p in results:
            pid = p["id"]
            title = p.get("title", "")
            space = p.get("space", {}).get("key", "")
            when = p.get("version", {}).get("when", "")
            html = p.get("body", {}).get("storage", {}).get("value", "")

            # Cursor tracking: compute (minute, id)
            dt = _parse_confluence_when(when)
            if dt:
                minute_str = _format_cql_minute(dt)
                pid_int = int(pid)

                if (max_minute is None) or (minute_str > max_minute):
                    max_minute = minute_str
                    max_id_in_minute = pid_int
                elif minute_str == max_minute:
                    if (max_id_in_minute is None) or (pid_int > max_id_in_minute):
                        max_id_in_minute = pid_int

            links = p.get("_links", {})
            webui = links.get("webui", "")
            page_url = f"{base_url}{webui}" if webui else base_url

            rec = {
                "id": pid,
                "title": title,
                "space": space,
                "updated": when,
                "url": page_url,
                "html": html
            }
            pages.append(rec)

            with open(os.path.join(raw_dir, f"{pid}.json"), "w", encoding="utf-8") as f:
                json.dump(rec, f, ensure_ascii=False)

        start += len(results)

        if total_size is not None and start >= total_size:
            break
        if len(results) < limit:
            break

    # Save meta
    meta = _load_meta(data_dir)
    meta["synced_at_epoch"] = time.time()
    meta["page_count_last_run"] = len(pages)

    # ✅ Advance cursor only if pages returned and incremental is enabled
    if cfg.get("incremental_sync", False) and max_minute is not None and max_id_in_minute is not None:
        meta["last_sync_cql_time"] = max_minute
        meta["last_sync_last_id"] = int(max_id_in_minute)

    _save_meta(data_dir, meta)
    return pages
