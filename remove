    pages = []
    start = 0
    total_size = None

    seen_batch_first_ids = set()
    max_batches = 500  # safety stop (adjust if needed)

    batch_num = 0
    while True:
        batch_num += 1
        if batch_num > max_batches:
            raise RuntimeError("Pagination safety stop: too many batches. Likely 'start' is being ignored.")

        url = f"{base_url}/rest/api/content/search"
        params = {
            "cql": effective_cql,
            "start": start,
            "limit": limit,
            "expand": "body.storage,version,space,_links"
        }

        r = requests.get(
            url,
            params=params,
            auth=auth,
            headers={"Accept": "application/json"},
            timeout=60,
            verify=False  # local dev only
        )

        if r.status_code == 400:
            raise RuntimeError(f"400 from Confluence. CQL was:\n{effective_cql}\n\nResponse:\n{r.text[:800]}")

        r.raise_for_status()
        payload = r.json()

        # Confluence usually includes totalSize/size
        if total_size is None:
            total_size = payload.get("totalSize")

        results = payload.get("results", [])
        size = payload.get("size", len(results))

        print(f"[SYNC] batch={batch_num} start={start} size={size} totalSize={total_size}")

        if not results:
            break

        # Detect repeating batches (start ignored / proxy caching)
        first_id = results[0].get("id")
        if first_id in seen_batch_first_ids:
            raise RuntimeError(
                "Pagination loop detected: received the same first page id again. "
                "This suggests Confluence is ignoring 'start' or returning cached results.\n"
                f"Repeated first_id={first_id}\n"
                f"Effective CQL={effective_cql}"
            )
        seen_batch_first_ids.add(first_id)

        for p in results:
            pid = p["id"]
            title = p.get("title", "")
            space = p.get("space", {}).get("key", "")
            when = p.get("version", {}).get("when", "")
            html = p.get("body", {}).get("storage", {}).get("value", "")

            links = p.get("_links", {})
            webui = links.get("webui", "")
            page_url = f"{base_url}{webui}" if webui else base_url

            rec = {
                "id": pid,
                "title": title,
                "space": space,
                "updated": when,
                "url": page_url,
                "html": html
            }
            pages.append(rec)

            with open(os.path.join(raw_dir, f"{pid}.json"), "w", encoding="utf-8") as f:
                json.dump(rec, f, ensure_ascii=False)

        # Advance pagination
        prev_start = start
        start += len(results)

        # If start didn't change, bail (should never happen, but safety)
        if start == prev_start:
            raise RuntimeError("Pagination did not advance (start unchanged).")

        # Robust stop if totalSize exists
        if total_size is not None and start >= total_size:
            break

        # Fallback stop: if fewer than limit returned
        if len(results) < limit:
            break
